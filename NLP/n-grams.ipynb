{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFT4zbqFwRmr"
   },
   "source": [
    "### **Bag of n_grams: Exercise**\n",
    "\n",
    "- Fake news refers to misinformation or disinformation in the country which is spread through word of mouth and more recently through digital communication such as What's app messages, social media posts, etc.\n",
    "\n",
    "- Fake news spreads faster than Real news and creates problems and fear among groups and in society.\n",
    "\n",
    "- We are going to address these problems using classical NLP techniques and going to classify whether a given message/ text is **Real or Fake Message**.\n",
    "\n",
    "- You will use a Bag of n-grams to pre-process the text and apply different classification algorithms.\n",
    "\n",
    "- Sklearn CountVectorizer has the inbuilt implementations for Bag of Words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBcs8GQb0C9_"
   },
   "source": [
    "### **About Data: Fake News Detection**\n",
    "\n",
    "Credits: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "\n",
    "- This data consists of two columns.\n",
    "        - Text\n",
    "        - label\n",
    "- Text is the statements or messages regarding a particular event/situation.\n",
    "\n",
    "- label feature tells whether the given Text is Fake or Real.\n",
    "\n",
    "- As there are only 2 classes, this problem comes under the **Binary Classification.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "KiYilX-lv_Vm",
    "outputId": "3a7bcc05-8e94-4d3d-c2a7-89d74f2b8202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  Fake  \n",
       "0  December 31, 2017  True  \n",
       "1  December 31, 2017  True  \n",
       "2  December 30, 2017  True  \n",
       "3  December 29, 2017  True  \n",
       "4  December 25, 2017  True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "#read the dataset with name \"Fake_Real_Data.csv\" and store it in a variable df\n",
    "df_Fake = pd.read_csv('Fake.csv')\n",
    "df_True = pd.read_csv('True.csv')\n",
    "\n",
    "df_Fake['Fake'] = True\n",
    "df_True['Fake'] = False \n",
    "\n",
    "df = pd.concat([df_Fake, df_True], axis=0, ignore_index= True)\n",
    "\n",
    "\n",
    "#print the shape of dataframe\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "#print top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N59dp0n1v_XU",
    "outputId": "3a438088-ff3f-4f57-9fbf-565b96693f02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fake\n",
       "True     23481\n",
       "False    21417\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the distribution of labels \n",
    "df.Fake.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev3bWDnA3tM-"
   },
   "source": [
    "### **Modelling without Pre-processing Text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hs94POE23Zjd"
   },
   "outputs": [],
   "source": [
    "#import train-test-split from sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too\n",
    "X = df.text\n",
    "y = df.Fake\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4I4S1PJG3ZlO",
    "outputId": "0d9865bd-48b9-4def-cfed-c740d6e02f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898,) and (44898,)\n"
     ]
    }
   ],
   "source": [
    "#print the shapes of X_train and X_test\n",
    "print(f'{X.shape} and {y.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2sO9uck4ILs"
   },
   "source": [
    "**Attempt 1** :\n",
    "\n",
    "1. using sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with unigram, bigram, and trigrams.\n",
    "- use KNN as the classifier with n_neighbors of 10 and metric as 'euclidean' distance.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLs6pmXE3Zou",
    "outputId": "1bbb4bf1-0b1e-4c5e-f751-fa4b331e772f"
   },
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU0g90Ra7BTW"
   },
   "source": [
    "**Attempt 2** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with unigram, bigram, and trigrams.\n",
    "- use **KNN** as the classifier with n_neighbors of 10 and metric as 'cosine' distance.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEsLSrIC3Zqf",
    "outputId": "b0354edc-1d3f-401b-c1ed-dd0cb7b769b4"
   },
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl5zoCbE8jds"
   },
   "source": [
    "\n",
    "**Attempt 3** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with only trigrams.\n",
    "- use **RandomForest** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bywjvZyv_ga",
    "outputId": "e2c93b51-8508-4c5a-b0ca-54e34ebe5075"
   },
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMeeE5zB8tZz"
   },
   "source": [
    "\n",
    "**Attempt 4** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with both unigram and bigrams.\n",
    "- use **Multinomial Naive Bayes** as the classifier with an alpha value of 0.75.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cP_zluNwBjS",
    "outputId": "108dd86a-5938-4040-9813-00b82d393ad1"
   },
   "outputs": [],
   "source": [
    "\n",
    "#1. create a pipeline object\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoFBbMga9tPB"
   },
   "source": [
    "<h3>Use text pre-processing to remove stop words, punctuations and apply lemmatization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R14_wUhGjqj5"
   },
   "outputs": [],
   "source": [
    "#use this utility function to get the preprocessed text data\n",
    "\n",
    "import spacy\n",
    "\n",
    "# load english language model and create nlp object from it\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def preprocess(text):\n",
    "    # remove stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JIKvTbl9jql0"
   },
   "outputs": [],
   "source": [
    "# create a new column \"preprocessed_txt\" and use the utility function above to get the clean data\n",
    "# this will take some time, please be patient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0O-uZncOjqpG",
    "outputId": "02d45596-aa7a-449d-dbba-3afd2bd8908b"
   },
   "outputs": [],
   "source": [
    "#print the top 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMVuYaYM-giF"
   },
   "source": [
    "**Build a model with pre processed text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "D25BcI45jqrE"
   },
   "outputs": [],
   "source": [
    "#Do the 'train-test' splitting with test size of 20% with random state of 2022 and stratify sampling too\n",
    "#Note: Make sure to use only the \"preprocessed_txt\" column for splitting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOh36PXR-nR_"
   },
   "source": [
    "**Let's check the scores with our best model till now**\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbfpQ5-kDgMt"
   },
   "source": [
    "**Attempt1** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with only trigrams.\n",
    "- use **RandomForest** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGQusE2rjquN",
    "outputId": "d1b83f99-0983-4feb-e24e-e3f9f2e09632"
   },
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB78pcAPEFQZ"
   },
   "source": [
    "**Attempt2** :\n",
    "\n",
    "1. using the sklearn pipeline module create a classification pipeline to classify the Data.\n",
    "\n",
    "**Note:**\n",
    "- using CountVectorizer with unigram, Bigram, and trigrams.\n",
    "- use **RandomForest** as the classifier.\n",
    "- print the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpwvD1mvjqvx",
    "outputId": "bbee2e0b-98da-4ae0-a480-259e0de8fa29"
   },
   "outputs": [],
   "source": [
    "#1. create a pipeline object\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "SLT0vKGRHAMF",
    "outputId": "540746d1-2dec-4585-918b-c4afba111e26"
   },
   "outputs": [],
   "source": [
    "#finally print the confusion matrix for the best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSfKnzG4EYSn"
   },
   "source": [
    "## **Please write down Final Observations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Solution**](./bag_of_n_grams_exercise_solutions.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bag_of_n_grams_exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
